{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1M84h0StW4s",
        "outputId": "57010955-d211-4f29-e2f0-fd3c4bd65dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete. Final Q-table:\n",
            "[[0.73509189 0.77378094 0.6983373  0.73509189]\n",
            " [0.73509189 0.         0.56656416 0.69149778]\n",
            " [0.68646594 0.         0.         0.40468869]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.77378094 0.81450625 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.90249769 0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.81450625 0.         0.857375   0.77378094]\n",
            " [0.81450625 0.9025     0.9025     0.        ]\n",
            " [0.857375   0.95       0.         0.85732504]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.90082281 0.95       0.85049776]\n",
            " [0.9025     0.95       1.         0.90249998]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Final Reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "# %pip install --upgrade gym\n",
        "# %pip install numpy==1.23.5\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Ensure compatibility with NumPy >= 1.24\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "# Create environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# Detect reset() return format\n",
        "def reset_env(env):\n",
        "    result = env.reset()\n",
        "    if isinstance(result, tuple):  # New Gym API\n",
        "        return result[0]\n",
        "    return result  # Old Gym API\n",
        "\n",
        "# Detect step() return format\n",
        "def step_env(env, action):\n",
        "    result = env.step(action)\n",
        "    if len(result) == 5:  # New Gym API\n",
        "        next_state, reward, terminated, truncated, info = result\n",
        "        return next_state, reward, (terminated or truncated), info\n",
        "    else:  # Old Gym API\n",
        "        next_state, reward, done, info = result\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.99\n",
        "episodes = 2000\n",
        "\n",
        "# Training\n",
        "for episode in range(episodes):\n",
        "    state = reset_env(env)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose action\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])     # Exploit\n",
        "\n",
        "        # Take action\n",
        "        next_state, reward, done, info = step_env(env, action)\n",
        "\n",
        "        # Update Q-table\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[next_state, :]) - Q[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(0.01, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"Training complete. Final Q-table:\")\n",
        "print(Q)\n",
        "\n",
        "# Testing trained agent\n",
        "state = reset_env(env)\n",
        "env.render()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(Q[state, :])\n",
        "    next_state, reward, done, info = step_env(env, action)\n",
        "    env.render()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Final Reward:\", total_reward)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "9EvK0od1tr6z",
        "outputId": "35f0aafe-b30f-4452-da7f-3ae13e2e0056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 - Reward: 19.0, Epsilon: 0.995\n",
            "Episode 1 - Reward: 55.0, Epsilon: 0.990\n",
            "Episode 2 - Reward: 16.0, Epsilon: 0.985\n",
            "Episode 3 - Reward: 9.0, Epsilon: 0.980\n",
            "Episode 4 - Reward: 30.0, Epsilon: 0.975\n",
            "Episode 5 - Reward: 41.0, Epsilon: 0.970\n",
            "Episode 6 - Reward: 8.0, Epsilon: 0.966\n",
            "Episode 7 - Reward: 66.0, Epsilon: 0.961\n",
            "Episode 8 - Reward: 56.0, Epsilon: 0.956\n",
            "Episode 9 - Reward: 22.0, Epsilon: 0.951\n",
            "Episode 10 - Reward: 28.0, Epsilon: 0.946\n",
            "Episode 11 - Reward: 15.0, Epsilon: 0.942\n",
            "Episode 12 - Reward: 17.0, Epsilon: 0.937\n",
            "Episode 13 - Reward: 10.0, Epsilon: 0.932\n",
            "Episode 14 - Reward: 13.0, Epsilon: 0.928\n",
            "Episode 15 - Reward: 26.0, Epsilon: 0.923\n",
            "Episode 16 - Reward: 31.0, Epsilon: 0.918\n",
            "Episode 17 - Reward: 37.0, Epsilon: 0.914\n",
            "Episode 18 - Reward: 24.0, Epsilon: 0.909\n",
            "Episode 19 - Reward: 11.0, Epsilon: 0.905\n",
            "Episode 20 - Reward: 15.0, Epsilon: 0.900\n",
            "Episode 21 - Reward: 37.0, Epsilon: 0.896\n",
            "Episode 22 - Reward: 20.0, Epsilon: 0.891\n",
            "Episode 23 - Reward: 46.0, Epsilon: 0.887\n",
            "Episode 24 - Reward: 15.0, Epsilon: 0.882\n",
            "Episode 25 - Reward: 16.0, Epsilon: 0.878\n",
            "Episode 26 - Reward: 21.0, Epsilon: 0.873\n",
            "Episode 27 - Reward: 36.0, Epsilon: 0.869\n",
            "Episode 28 - Reward: 25.0, Epsilon: 0.865\n",
            "Episode 29 - Reward: 21.0, Epsilon: 0.860\n",
            "Episode 30 - Reward: 17.0, Epsilon: 0.856\n",
            "Episode 31 - Reward: 50.0, Epsilon: 0.852\n",
            "Episode 32 - Reward: 13.0, Epsilon: 0.848\n",
            "Episode 33 - Reward: 24.0, Epsilon: 0.843\n",
            "Episode 34 - Reward: 56.0, Epsilon: 0.839\n",
            "Episode 35 - Reward: 14.0, Epsilon: 0.835\n",
            "Episode 36 - Reward: 31.0, Epsilon: 0.831\n",
            "Episode 37 - Reward: 14.0, Epsilon: 0.827\n",
            "Episode 38 - Reward: 21.0, Epsilon: 0.822\n",
            "Episode 39 - Reward: 22.0, Epsilon: 0.818\n",
            "Episode 40 - Reward: 15.0, Epsilon: 0.814\n",
            "Episode 41 - Reward: 74.0, Epsilon: 0.810\n",
            "Episode 42 - Reward: 23.0, Epsilon: 0.806\n",
            "Episode 43 - Reward: 66.0, Epsilon: 0.802\n",
            "Episode 44 - Reward: 33.0, Epsilon: 0.798\n",
            "Episode 45 - Reward: 54.0, Epsilon: 0.794\n",
            "Episode 46 - Reward: 25.0, Epsilon: 0.790\n",
            "Episode 47 - Reward: 13.0, Epsilon: 0.786\n",
            "Episode 48 - Reward: 42.0, Epsilon: 0.782\n",
            "Episode 49 - Reward: 9.0, Epsilon: 0.778\n",
            "Episode 50 - Reward: 61.0, Epsilon: 0.774\n",
            "Episode 51 - Reward: 9.0, Epsilon: 0.771\n",
            "Episode 52 - Reward: 11.0, Epsilon: 0.767\n",
            "Episode 53 - Reward: 66.0, Epsilon: 0.763\n",
            "Episode 54 - Reward: 48.0, Epsilon: 0.759\n",
            "Episode 55 - Reward: 42.0, Epsilon: 0.755\n",
            "Episode 56 - Reward: 14.0, Epsilon: 0.751\n",
            "Episode 57 - Reward: 29.0, Epsilon: 0.748\n",
            "Episode 58 - Reward: 40.0, Epsilon: 0.744\n",
            "Episode 59 - Reward: 14.0, Epsilon: 0.740\n",
            "Episode 60 - Reward: 57.0, Epsilon: 0.737\n",
            "Episode 61 - Reward: 65.0, Epsilon: 0.733\n",
            "Episode 62 - Reward: 85.0, Epsilon: 0.729\n",
            "Episode 63 - Reward: 106.0, Epsilon: 0.726\n",
            "Episode 64 - Reward: 74.0, Epsilon: 0.722\n",
            "Episode 65 - Reward: 151.0, Epsilon: 0.718\n",
            "Episode 66 - Reward: 26.0, Epsilon: 0.715\n",
            "Episode 67 - Reward: 41.0, Epsilon: 0.711\n",
            "Episode 68 - Reward: 50.0, Epsilon: 0.708\n",
            "Episode 69 - Reward: 53.0, Epsilon: 0.704\n",
            "Episode 70 - Reward: 52.0, Epsilon: 0.701\n",
            "Episode 71 - Reward: 28.0, Epsilon: 0.697\n",
            "Episode 72 - Reward: 38.0, Epsilon: 0.694\n",
            "Episode 73 - Reward: 24.0, Epsilon: 0.690\n",
            "Episode 74 - Reward: 53.0, Epsilon: 0.687\n",
            "Episode 75 - Reward: 36.0, Epsilon: 0.683\n",
            "Episode 76 - Reward: 38.0, Epsilon: 0.680\n",
            "Episode 77 - Reward: 29.0, Epsilon: 0.676\n",
            "Episode 78 - Reward: 83.0, Epsilon: 0.673\n",
            "Episode 79 - Reward: 48.0, Epsilon: 0.670\n",
            "Episode 80 - Reward: 48.0, Epsilon: 0.666\n",
            "Episode 81 - Reward: 21.0, Epsilon: 0.663\n",
            "Episode 82 - Reward: 17.0, Epsilon: 0.660\n",
            "Episode 83 - Reward: 163.0, Epsilon: 0.656\n",
            "Episode 84 - Reward: 102.0, Epsilon: 0.653\n",
            "Episode 85 - Reward: 25.0, Epsilon: 0.650\n",
            "Episode 86 - Reward: 31.0, Epsilon: 0.647\n",
            "Episode 87 - Reward: 23.0, Epsilon: 0.643\n",
            "Episode 88 - Reward: 13.0, Epsilon: 0.640\n",
            "Episode 89 - Reward: 30.0, Epsilon: 0.637\n",
            "Episode 90 - Reward: 21.0, Epsilon: 0.634\n",
            "Episode 91 - Reward: 66.0, Epsilon: 0.631\n",
            "Episode 92 - Reward: 16.0, Epsilon: 0.627\n",
            "Episode 93 - Reward: 43.0, Epsilon: 0.624\n",
            "Episode 94 - Reward: 82.0, Epsilon: 0.621\n",
            "Episode 95 - Reward: 11.0, Epsilon: 0.618\n",
            "Episode 96 - Reward: 49.0, Epsilon: 0.615\n",
            "Episode 97 - Reward: 12.0, Epsilon: 0.612\n",
            "Episode 98 - Reward: 36.0, Epsilon: 0.609\n",
            "Episode 99 - Reward: 17.0, Epsilon: 0.606\n",
            "Episode 100 - Reward: 201.0, Epsilon: 0.603\n",
            "Episode 101 - Reward: 15.0, Epsilon: 0.600\n",
            "Episode 102 - Reward: 133.0, Epsilon: 0.597\n",
            "Episode 103 - Reward: 55.0, Epsilon: 0.594\n",
            "Episode 104 - Reward: 15.0, Epsilon: 0.591\n",
            "Episode 105 - Reward: 134.0, Epsilon: 0.588\n",
            "Episode 106 - Reward: 151.0, Epsilon: 0.585\n",
            "Episode 107 - Reward: 64.0, Epsilon: 0.582\n",
            "Episode 108 - Reward: 19.0, Epsilon: 0.579\n",
            "Episode 109 - Reward: 61.0, Epsilon: 0.576\n",
            "Episode 110 - Reward: 62.0, Epsilon: 0.573\n",
            "Episode 111 - Reward: 10.0, Epsilon: 0.570\n",
            "Episode 112 - Reward: 18.0, Epsilon: 0.568\n",
            "Episode 113 - Reward: 109.0, Epsilon: 0.565\n",
            "Episode 114 - Reward: 148.0, Epsilon: 0.562\n",
            "Episode 115 - Reward: 142.0, Epsilon: 0.559\n",
            "Episode 116 - Reward: 119.0, Epsilon: 0.556\n",
            "Episode 117 - Reward: 60.0, Epsilon: 0.554\n",
            "Episode 118 - Reward: 109.0, Epsilon: 0.551\n",
            "Episode 119 - Reward: 131.0, Epsilon: 0.548\n",
            "Episode 120 - Reward: 96.0, Epsilon: 0.545\n",
            "Episode 121 - Reward: 39.0, Epsilon: 0.543\n",
            "Episode 122 - Reward: 33.0, Epsilon: 0.540\n",
            "Episode 123 - Reward: 34.0, Epsilon: 0.537\n",
            "Episode 124 - Reward: 172.0, Epsilon: 0.534\n",
            "Episode 125 - Reward: 87.0, Epsilon: 0.532\n",
            "Episode 126 - Reward: 220.0, Epsilon: 0.529\n",
            "Episode 127 - Reward: 49.0, Epsilon: 0.526\n",
            "Episode 128 - Reward: 107.0, Epsilon: 0.524\n",
            "Episode 129 - Reward: 117.0, Epsilon: 0.521\n",
            "Episode 130 - Reward: 95.0, Epsilon: 0.519\n",
            "Episode 131 - Reward: 46.0, Epsilon: 0.516\n",
            "Episode 132 - Reward: 46.0, Epsilon: 0.513\n",
            "Episode 133 - Reward: 217.0, Epsilon: 0.511\n",
            "Episode 134 - Reward: 13.0, Epsilon: 0.508\n",
            "Episode 135 - Reward: 50.0, Epsilon: 0.506\n",
            "Episode 136 - Reward: 70.0, Epsilon: 0.503\n",
            "Episode 137 - Reward: 123.0, Epsilon: 0.501\n",
            "Episode 138 - Reward: 20.0, Epsilon: 0.498\n",
            "Episode 139 - Reward: 85.0, Epsilon: 0.496\n",
            "Episode 140 - Reward: 111.0, Epsilon: 0.493\n",
            "Episode 141 - Reward: 23.0, Epsilon: 0.491\n",
            "Episode 142 - Reward: 130.0, Epsilon: 0.488\n",
            "Episode 143 - Reward: 93.0, Epsilon: 0.486\n",
            "Episode 144 - Reward: 204.0, Epsilon: 0.483\n",
            "Episode 145 - Reward: 134.0, Epsilon: 0.481\n",
            "Episode 146 - Reward: 56.0, Epsilon: 0.479\n",
            "Episode 147 - Reward: 38.0, Epsilon: 0.476\n",
            "Episode 148 - Reward: 42.0, Epsilon: 0.474\n",
            "Episode 149 - Reward: 260.0, Epsilon: 0.471\n",
            "Episode 150 - Reward: 250.0, Epsilon: 0.469\n",
            "Episode 151 - Reward: 53.0, Epsilon: 0.467\n",
            "Episode 152 - Reward: 105.0, Epsilon: 0.464\n",
            "Episode 153 - Reward: 63.0, Epsilon: 0.462\n",
            "Episode 154 - Reward: 28.0, Epsilon: 0.460\n",
            "Episode 155 - Reward: 74.0, Epsilon: 0.458\n",
            "Episode 156 - Reward: 229.0, Epsilon: 0.455\n",
            "Episode 157 - Reward: 157.0, Epsilon: 0.453\n",
            "Episode 158 - Reward: 23.0, Epsilon: 0.451\n",
            "Episode 159 - Reward: 22.0, Epsilon: 0.448\n",
            "Episode 160 - Reward: 86.0, Epsilon: 0.446\n",
            "Episode 161 - Reward: 17.0, Epsilon: 0.444\n",
            "Episode 162 - Reward: 209.0, Epsilon: 0.442\n",
            "Episode 163 - Reward: 124.0, Epsilon: 0.440\n",
            "Episode 164 - Reward: 100.0, Epsilon: 0.437\n",
            "Episode 165 - Reward: 99.0, Epsilon: 0.435\n",
            "Episode 166 - Reward: 80.0, Epsilon: 0.433\n",
            "Episode 167 - Reward: 125.0, Epsilon: 0.431\n",
            "Episode 168 - Reward: 204.0, Epsilon: 0.429\n",
            "Episode 169 - Reward: 67.0, Epsilon: 0.427\n",
            "Episode 170 - Reward: 52.0, Epsilon: 0.424\n",
            "Episode 171 - Reward: 31.0, Epsilon: 0.422\n",
            "Episode 172 - Reward: 23.0, Epsilon: 0.420\n",
            "Episode 173 - Reward: 145.0, Epsilon: 0.418\n",
            "Episode 174 - Reward: 126.0, Epsilon: 0.416\n",
            "Episode 175 - Reward: 137.0, Epsilon: 0.414\n",
            "Episode 176 - Reward: 45.0, Epsilon: 0.412\n",
            "Episode 177 - Reward: 159.0, Epsilon: 0.410\n",
            "Episode 178 - Reward: 37.0, Epsilon: 0.408\n",
            "Episode 179 - Reward: 182.0, Epsilon: 0.406\n",
            "Episode 180 - Reward: 176.0, Epsilon: 0.404\n",
            "Episode 181 - Reward: 16.0, Epsilon: 0.402\n",
            "Episode 182 - Reward: 167.0, Epsilon: 0.400\n",
            "Episode 183 - Reward: 121.0, Epsilon: 0.398\n",
            "Episode 184 - Reward: 34.0, Epsilon: 0.396\n",
            "Episode 185 - Reward: 49.0, Epsilon: 0.394\n",
            "Episode 186 - Reward: 245.0, Epsilon: 0.392\n",
            "Episode 187 - Reward: 103.0, Epsilon: 0.390\n",
            "Episode 188 - Reward: 175.0, Epsilon: 0.388\n",
            "Episode 189 - Reward: 244.0, Epsilon: 0.386\n",
            "Episode 190 - Reward: 144.0, Epsilon: 0.384\n",
            "Episode 191 - Reward: 141.0, Epsilon: 0.382\n",
            "Episode 192 - Reward: 17.0, Epsilon: 0.380\n",
            "Episode 193 - Reward: 63.0, Epsilon: 0.378\n",
            "Episode 194 - Reward: 34.0, Epsilon: 0.376\n",
            "Episode 195 - Reward: 135.0, Epsilon: 0.374\n",
            "Episode 196 - Reward: 55.0, Epsilon: 0.373\n",
            "Episode 197 - Reward: 107.0, Epsilon: 0.371\n",
            "Episode 198 - Reward: 73.0, Epsilon: 0.369\n",
            "Episode 199 - Reward: 72.0, Epsilon: 0.367\n",
            "Episode 200 - Reward: 122.0, Epsilon: 0.365\n",
            "Episode 201 - Reward: 49.0, Epsilon: 0.363\n",
            "Episode 202 - Reward: 36.0, Epsilon: 0.361\n",
            "Episode 203 - Reward: 119.0, Epsilon: 0.360\n",
            "Episode 204 - Reward: 135.0, Epsilon: 0.358\n",
            "Episode 205 - Reward: 61.0, Epsilon: 0.356\n",
            "Episode 206 - Reward: 128.0, Epsilon: 0.354\n",
            "Episode 207 - Reward: 134.0, Epsilon: 0.353\n",
            "Episode 208 - Reward: 82.0, Epsilon: 0.351\n",
            "Episode 209 - Reward: 68.0, Epsilon: 0.349\n",
            "Episode 210 - Reward: 86.0, Epsilon: 0.347\n",
            "Episode 211 - Reward: 122.0, Epsilon: 0.346\n",
            "Episode 212 - Reward: 45.0, Epsilon: 0.344\n",
            "Episode 213 - Reward: 136.0, Epsilon: 0.342\n",
            "Episode 214 - Reward: 120.0, Epsilon: 0.340\n",
            "Episode 215 - Reward: 46.0, Epsilon: 0.339\n",
            "Episode 216 - Reward: 120.0, Epsilon: 0.337\n",
            "Episode 217 - Reward: 119.0, Epsilon: 0.335\n",
            "Episode 218 - Reward: 65.0, Epsilon: 0.334\n",
            "Episode 219 - Reward: 106.0, Epsilon: 0.332\n",
            "Episode 220 - Reward: 108.0, Epsilon: 0.330\n",
            "Episode 221 - Reward: 80.0, Epsilon: 0.329\n",
            "Episode 222 - Reward: 56.0, Epsilon: 0.327\n",
            "Episode 223 - Reward: 142.0, Epsilon: 0.325\n",
            "Episode 224 - Reward: 83.0, Epsilon: 0.324\n",
            "Episode 225 - Reward: 113.0, Epsilon: 0.322\n",
            "Episode 226 - Reward: 223.0, Epsilon: 0.321\n",
            "Episode 227 - Reward: 156.0, Epsilon: 0.319\n",
            "Episode 228 - Reward: 98.0, Epsilon: 0.317\n",
            "Episode 229 - Reward: 106.0, Epsilon: 0.316\n",
            "Episode 230 - Reward: 117.0, Epsilon: 0.314\n",
            "Episode 231 - Reward: 55.0, Epsilon: 0.313\n",
            "Episode 232 - Reward: 128.0, Epsilon: 0.311\n",
            "Episode 233 - Reward: 71.0, Epsilon: 0.309\n",
            "Episode 234 - Reward: 32.0, Epsilon: 0.308\n",
            "Episode 235 - Reward: 79.0, Epsilon: 0.306\n",
            "Episode 236 - Reward: 30.0, Epsilon: 0.305\n",
            "Episode 237 - Reward: 49.0, Epsilon: 0.303\n",
            "Episode 238 - Reward: 26.0, Epsilon: 0.302\n",
            "Episode 239 - Reward: 44.0, Epsilon: 0.300\n",
            "Episode 240 - Reward: 41.0, Epsilon: 0.299\n",
            "Episode 241 - Reward: 86.0, Epsilon: 0.297\n",
            "Episode 242 - Reward: 18.0, Epsilon: 0.296\n",
            "Episode 243 - Reward: 74.0, Epsilon: 0.294\n",
            "Episode 244 - Reward: 131.0, Epsilon: 0.293\n",
            "Episode 245 - Reward: 103.0, Epsilon: 0.291\n",
            "Episode 246 - Reward: 117.0, Epsilon: 0.290\n",
            "Episode 247 - Reward: 75.0, Epsilon: 0.288\n",
            "Episode 248 - Reward: 98.0, Epsilon: 0.287\n",
            "Episode 249 - Reward: 100.0, Epsilon: 0.286\n",
            "Episode 250 - Reward: 15.0, Epsilon: 0.284\n",
            "Episode 251 - Reward: 99.0, Epsilon: 0.283\n",
            "Episode 252 - Reward: 131.0, Epsilon: 0.281\n",
            "Episode 253 - Reward: 103.0, Epsilon: 0.280\n",
            "Episode 254 - Reward: 99.0, Epsilon: 0.279\n",
            "Episode 255 - Reward: 140.0, Epsilon: 0.277\n",
            "Episode 256 - Reward: 101.0, Epsilon: 0.276\n",
            "Episode 257 - Reward: 107.0, Epsilon: 0.274\n",
            "Episode 258 - Reward: 38.0, Epsilon: 0.273\n",
            "Episode 259 - Reward: 110.0, Epsilon: 0.272\n",
            "Episode 260 - Reward: 78.0, Epsilon: 0.270\n",
            "Episode 261 - Reward: 55.0, Epsilon: 0.269\n",
            "Episode 262 - Reward: 104.0, Epsilon: 0.268\n",
            "Episode 263 - Reward: 140.0, Epsilon: 0.266\n",
            "Episode 264 - Reward: 103.0, Epsilon: 0.265\n",
            "Episode 265 - Reward: 84.0, Epsilon: 0.264\n",
            "Episode 266 - Reward: 70.0, Epsilon: 0.262\n",
            "Episode 267 - Reward: 42.0, Epsilon: 0.261\n",
            "Episode 268 - Reward: 13.0, Epsilon: 0.260\n",
            "Episode 269 - Reward: 88.0, Epsilon: 0.258\n",
            "Episode 270 - Reward: 124.0, Epsilon: 0.257\n",
            "Episode 271 - Reward: 21.0, Epsilon: 0.256\n",
            "Episode 272 - Reward: 128.0, Epsilon: 0.255\n",
            "Episode 273 - Reward: 96.0, Epsilon: 0.253\n",
            "Episode 274 - Reward: 144.0, Epsilon: 0.252\n",
            "Episode 275 - Reward: 170.0, Epsilon: 0.251\n",
            "Episode 276 - Reward: 102.0, Epsilon: 0.249\n",
            "Episode 277 - Reward: 63.0, Epsilon: 0.248\n",
            "Episode 278 - Reward: 35.0, Epsilon: 0.247\n",
            "Episode 279 - Reward: 16.0, Epsilon: 0.246\n",
            "Episode 280 - Reward: 226.0, Epsilon: 0.245\n",
            "Episode 281 - Reward: 134.0, Epsilon: 0.243\n",
            "Episode 282 - Reward: 18.0, Epsilon: 0.242\n",
            "Episode 283 - Reward: 85.0, Epsilon: 0.241\n",
            "Episode 284 - Reward: 104.0, Epsilon: 0.240\n",
            "Episode 285 - Reward: 14.0, Epsilon: 0.238\n",
            "Episode 286 - Reward: 94.0, Epsilon: 0.237\n",
            "Episode 287 - Reward: 44.0, Epsilon: 0.236\n",
            "Episode 288 - Reward: 104.0, Epsilon: 0.235\n",
            "Episode 289 - Reward: 60.0, Epsilon: 0.234\n",
            "Episode 290 - Reward: 47.0, Epsilon: 0.233\n",
            "Episode 291 - Reward: 168.0, Epsilon: 0.231\n",
            "Episode 292 - Reward: 27.0, Epsilon: 0.230\n",
            "Episode 293 - Reward: 164.0, Epsilon: 0.229\n",
            "Episode 294 - Reward: 20.0, Epsilon: 0.228\n",
            "Episode 295 - Reward: 38.0, Epsilon: 0.227\n",
            "Episode 296 - Reward: 138.0, Epsilon: 0.226\n",
            "Episode 297 - Reward: 110.0, Epsilon: 0.225\n",
            "Episode 298 - Reward: 102.0, Epsilon: 0.223\n",
            "Episode 299 - Reward: 93.0, Epsilon: 0.222\n",
            "Episode 300 - Reward: 95.0, Epsilon: 0.221\n",
            "Episode 301 - Reward: 56.0, Epsilon: 0.220\n",
            "Episode 302 - Reward: 159.0, Epsilon: 0.219\n",
            "Episode 303 - Reward: 128.0, Epsilon: 0.218\n",
            "Episode 304 - Reward: 113.0, Epsilon: 0.217\n",
            "Episode 305 - Reward: 42.0, Epsilon: 0.216\n",
            "Episode 306 - Reward: 54.0, Epsilon: 0.215\n",
            "Episode 307 - Reward: 30.0, Epsilon: 0.214\n",
            "Episode 308 - Reward: 136.0, Epsilon: 0.212\n",
            "Episode 309 - Reward: 167.0, Epsilon: 0.211\n",
            "Episode 310 - Reward: 15.0, Epsilon: 0.210\n",
            "Episode 311 - Reward: 20.0, Epsilon: 0.209\n",
            "Episode 312 - Reward: 101.0, Epsilon: 0.208\n",
            "Episode 313 - Reward: 24.0, Epsilon: 0.207\n",
            "Episode 314 - Reward: 106.0, Epsilon: 0.206\n",
            "Episode 315 - Reward: 139.0, Epsilon: 0.205\n",
            "Episode 316 - Reward: 80.0, Epsilon: 0.204\n",
            "Episode 317 - Reward: 85.0, Epsilon: 0.203\n",
            "Episode 318 - Reward: 103.0, Epsilon: 0.202\n",
            "Episode 319 - Reward: 92.0, Epsilon: 0.201\n",
            "Episode 320 - Reward: 110.0, Epsilon: 0.200\n",
            "Episode 321 - Reward: 100.0, Epsilon: 0.199\n",
            "Episode 322 - Reward: 104.0, Epsilon: 0.198\n",
            "Episode 323 - Reward: 104.0, Epsilon: 0.197\n",
            "Episode 324 - Reward: 31.0, Epsilon: 0.196\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mzip\u001b[39m(*minibatch)\n\u001b[32m     82\u001b[39m states = torch.FloatTensor(np.vstack(states))\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m actions = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m rewards = torch.FloatTensor(rewards)\n\u001b[32m     85\u001b[39m next_states = torch.FloatTensor(np.vstack(next_states))\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# %pip install numpy==1.23.5\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Hyperparameters\n",
        "EPISODES = 500\n",
        "GAMMA = 0.95\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# Q-Network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 24)\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Replay Buffer\n",
        "memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Networks\n",
        "policy_net = DQN(state_size, action_size)\n",
        "target_net = DQN(state_size, action_size)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "# Training Loop\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    if isinstance(state, tuple):  # gym API fix\n",
        "        state = state[0]\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Epsilon-greedy\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randrange(action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(policy_net(torch.FloatTensor(state))).item()\n",
        "\n",
        "        # Step\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        # Store experience\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Train from memory\n",
        "        if len(memory) >= BATCH_SIZE:\n",
        "            minibatch = random.sample(memory, BATCH_SIZE)\n",
        "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "            states = torch.FloatTensor(np.vstack(states))\n",
        "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
        "            rewards = torch.FloatTensor(rewards)\n",
        "            next_states = torch.FloatTensor(np.vstack(next_states))\n",
        "            dones = torch.FloatTensor(dones)\n",
        "\n",
        "            # Q values\n",
        "            current_q = policy_net(states).gather(1, actions).squeeze()\n",
        "            next_q = target_net(next_states).max(1)[0]\n",
        "            target_q = rewards + (GAMMA * next_q * (1 - dones))\n",
        "\n",
        "            # Loss and optimize\n",
        "            loss = nn.MSELoss()(current_q, target_q.detach())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "\n",
        "    # Update target network\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode} - Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
