{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UynOo1tTImZ"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"   # disable wandb\n",
        "\n",
        "# 1. Load model + tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. Apply LoRA config\n",
        "peft_config = LoraConfig(task_type=\"CAUSAL_LM\", r=8, lora_alpha=16, lora_dropout=0.1)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 3. Tiny dataset with labels\n",
        "def preprocess(batch):\n",
        "    tokens = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\", split=\"train[:200]\").map(preprocess, batched=True)\n",
        "\n",
        "# 4. Training\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"out\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "trainer = Trainer(model=model, args=args, train_dataset=dataset)\n",
        "trainer.train()\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model + tokenizer from Trainer\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0  # set -1 if running on CPU\n",
        ")\n",
        "\n",
        "# Try with a custom prompt\n",
        "prompt = \"If i would be the Prime Minister of India..\"\n",
        "outputs = text_generator(prompt, max_length=50, num_return_sequences=2, do_sample=True, top_k=50)\n",
        "\n",
        "# Print results\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"\\nGenerated {i+1}: {out['generated_text']}\")\n",
        "\n"
      ]
    }
  ]
}